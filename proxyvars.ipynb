{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype of detection tool for proxy variables in datasets\n",
    "\n",
    "**Objective:** The aim is to create a software tool capable of checking datasets for the presence of proxy variables, with an initial focus on detecting gender bias. Over time, the goal is to extend the tool to identify other forms of bias. By doing so, this tool will help integrate AI Governance into practice, making compliance with regulations practically feasible.\n",
    "\n",
    "**Motivation:** It's crucial to develop a tool that can scan datasets for proxy variables to detect bias. From a legal perspective (EU/NL), everyone must be treated equally in comparable situations, and discrimination based on religion, belief, political affiliation, race (ethnicity), gender, or any other grounds is prohibited. Furthermore, specific laws criminalize such forms of discrimination. Given recent social discussions (EU/NL), socio-economic status or level of education could also be considered; however, these are not currently prioritized as they are not explicitly mentioned in the law.\n",
    "\n",
    "**Target Audience:** This tool is designed for developers and users of AI systems. The AI Act mandates that developers and users take measures to ensure their systems do not produce discriminatory effects. Detecting bias in datasets is essential to comply with these legal requirements and to ensure fair and equal treatment. To support developers, the Dutch Government has created a report detailing the leading questions and principles for developing and implementing AI systems that comply with non-discrimination laws from a legal, technical, and organizational perspective. This guide, \"Non-Discrimination in Artificial Intelligence\" (link), also includes examples of proxy variables.\n",
    "\n",
    "## Relevance in the context of AI Governance\n",
    "\n",
    "AI Governance aims to ensure that AI systems align with the organization's goals while adhering to societal objectives and legal requirements. One of the initial steps in AI system development is selecting the training and input datasets. This critical first step often harbors potential issues that can conflict with these requirements.\n",
    "\n",
    "Developers of AI and ICT systems frequently lack a deep understanding of the algorithms they employ, particularly those they didn't develop themselves. This is especially true for machine learning algorithms, where the patterns identified by the algorithm often remain opaque. New EU regulations mandate that developers—and their users—enhance their understanding of these technologies, emphasizing AI literacy. However, even with this knowledge, making unbiased decisions remains challenging. Bias can persist despite the best intentions, and this issue is evident in contexts like exam assessments.\n",
    "\n",
    "Legally, there is an obligation to treat men and women equally in comparable situations where gender is the only difference. Ideally, any potential bias should be identified early in the development process to prevent any adverse effects or negative media coverage. The law prohibits algorithms (or expert rules) from making explicit, easily recognizable distinctions between men and women. The challenge lies in uncovering implicit biases, which stem from the patterns detected within the dataset. Excluding gender from the training or input data does not necessarily eliminate the risk of discrimination.\n",
    "\n",
    "This is because, in practice, the values for other variables between men and women are often unevenly distributed. For instance, age structure varies across regions, with more women in older age groups. Additionally, work experience can differ due to women often continuing their studies and taking parental leave, which men typically don't experience. Including criteria such as age, education level, or work experience in assessments implicitly includes gender, as algorithms can differentiate between men and women based on these factors.\n",
    "\n",
    "When men and women are judged differently in society—unfortunately a common scenario—this bias is reflected in historical datasets. These historical decisions are used as labels when training and validating algorithms or models, leading to men and women being placed into different clusters based on proxy variables. Consequently, the existing bias is perpetuated within the AI system.\n",
    "\n",
    "From experience, literature, and press releases, many proxy variables have been identified. Statistical agencies maintain lists of proxy variables for their analyses. Proxy variables are commonly used to estimate the value of variables that aren't directly measurable, such as the prevalence of mental health issues within a population. They are also instrumental in estimating the underrepresentation of certain groups and assessing the representativeness of responses in self-reporting. In quantitative studies, proxy variables are well-known tools that help prevent unintended bias and misrepresentation.\n",
    "\n",
    "Developers may be aware of the presence of proxy variables in their datasets, but this awareness is often insufficient in practice. Proxy variables can change over time and vary by region. For instance, the gap in educational attainment between men and women in the EU has reversed over the past 40 years. Historically, men were more likely to be highly educated, but now, in many regions, women hold this advantage. Parental leave, once predominantly taken by women, is now more frequently taken by men.\n",
    "\n",
    "It is essential to continuously monitor social trends and enable AI systems to understand how these trends manifest locally. The use of proxy variables cannot always be avoided, similar to sensitive variables, as long as their necessity and significance for the intended purpose are carefully considered. Awareness of the proxy nature of particular variables allows for measures to prevent these variables from leading to legally prohibited gender discrimination. Identifying and preventing bias in an AI system early on is a key aspect of AI Governance in practice.\n",
    "\n",
    "Recognizing a proxy variable is particularly challenging when its discriminatory nature is subtle. A notable example is the recruitment tool developed by Amazon. This classification algorithm analyzed CV content, but due to differences in terminology used by men and women (e.g., \"women's rowing club\"), and because men had historically been selected more often, the algorithm assigned women a lower chance of suitability. As a result, candidates with identical qualifications could be treated differently based on gender. Due to the significant impact of such systems and the frequent oversight of these design flaws by developers, all AI systems used for recruitment are classified as high risk in the European Union and are subject to additional scrutiny and regulation.\n",
    "\n",
    "## Technical Approach...\n",
    "\n",
    "To identify proxy variables, we rely on a dataset's description. It is not always necessary to analyze the records within the dataset itself if the description is sufficiently detailed. We use descriptions similar to those found on Kaggle, focusing on a general overview of the content, the population, and the meaning of each column. Additionally, understanding the intended use of the dataset is crucial. If the actual data is available, further information can be gathered through dialogue with the user, although this is not essential for the initial version of the tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prototype tool is utilizing llamam-server () with either the Llama 3.1 8B or Mistal 7B model, with a Java wrapper that sends HTTP request in a specific style to ensure the result is in machine interpretable JSON format. The input data is a dataset description in the style as it commonly used on Kaggle. This is natural language text in a relative free format that typically briefly describes the intended goal of the dataset, its original and a brief description of each column, or variable, in the dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
