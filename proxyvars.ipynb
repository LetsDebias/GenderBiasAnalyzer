{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prototype of detection tool for proxy variables in datasets\n",
    "\n",
    "**Objective:** The aim is to create a software tool capable of checking datasets for the presence of proxy variables, with an initial focus on detecting gender bias. Over time, the goal is to extend the tool to identify other forms of bias. By doing so, this tool will help integrate AI Governance into practice, making compliance with regulations practically feasible.\n",
    "\n",
    "**Motivation:** It's crucial to develop a tool that can scan datasets for proxy variables to detect bias. From a legal perspective (EU/NL), everyone must be treated equally in comparable situations, and discrimination based on religion, belief, political affiliation, race (ethnicity), gender, or any other grounds is prohibited. Furthermore, specific laws criminalize such forms of discrimination. Given recent social discussions (EU/NL), socio-economic status or level of education could also be considered; however, these are not currently prioritized as they are not explicitly mentioned in the law.\n",
    "\n",
    "**Target Audience:** This tool is designed for developers and users of AI systems. The AI Act mandates that developers and users take measures to ensure their systems do not produce discriminatory effects. Detecting bias in datasets is essential to comply with these legal requirements and to ensure fair and equal treatment. To support developers, the Dutch Government has created a report detailing the leading questions and principles for developing and implementing AI systems that comply with non-discrimination laws from a legal, technical, and organizational perspective. This guide, \"Non-Discrimination in Artificial Intelligence\" (link), also includes examples of proxy variables.\n",
    "\n",
    "## Relevance in the context of AI Governance\n",
    "\n",
    "AI Governance aims to ensure that AI systems align with the organization's goals while adhering to societal objectives and legal requirements. One of the initial steps in AI system development is selecting the training and input datasets. This critical first step often harbors potential issues that can conflict with these requirements.\n",
    "\n",
    "Developers of AI and ICT systems frequently lack a deep understanding of the algorithms they employ, particularly those they didn't develop themselves. This is especially true for machine learning algorithms, where the patterns identified by the algorithm often remain opaque. New EU regulations mandate that developers—and their users—enhance their understanding of these technologies, emphasizing AI literacy. However, even with this knowledge, making unbiased decisions remains challenging. Bias can persist despite the best intentions, and this issue is evident in contexts like exam assessments.\n",
    "\n",
    "Legally, there is an obligation to treat men and women equally in comparable situations where gender is the only difference. Ideally, any potential bias should be identified early in the development process to prevent any adverse effects or negative media coverage. The law prohibits algorithms (or expert rules) from making explicit, easily recognizable distinctions between men and women. The challenge lies in uncovering implicit biases, which stem from the patterns detected within the dataset. Excluding gender from the training or input data does not necessarily eliminate the risk of discrimination.\n",
    "\n",
    "This is because, in practice, the values for other variables between men and women are often unevenly distributed. For instance, age structure varies across regions, with more women in older age groups. Additionally, work experience can differ due to women often continuing their studies and taking parental leave, which men typically don't experience. Including criteria such as age, education level, or work experience in assessments implicitly includes gender, as algorithms can differentiate between men and women based on these factors.\n",
    "\n",
    "When men and women are judged differently in society—unfortunately a common scenario—this bias is reflected in historical datasets. These historical decisions are used as labels when training and validating algorithms or models, leading to men and women being placed into different clusters based on proxy variables. Consequently, the existing bias is perpetuated within the AI system.\n",
    "\n",
    "From experience, literature, and press releases, many proxy variables have been identified. Statistical agencies maintain lists of proxy variables for their analyses. Proxy variables are commonly used to estimate the value of variables that aren't directly measurable, such as the prevalence of mental health issues within a population. They are also instrumental in estimating the underrepresentation of certain groups and assessing the representativeness of responses in self-reporting. In quantitative studies, proxy variables are well-known tools that help prevent unintended bias and misrepresentation.\n",
    "\n",
    "Developers may be aware of the presence of proxy variables in their datasets, but this awareness is often insufficient in practice. Proxy variables can change over time and vary by region. For instance, the gap in educational attainment between men and women in the EU has reversed over the past 40 years. Historically, men were more likely to be highly educated, but now, in many regions, women hold this advantage. Parental leave, once predominantly taken by women, is now more frequently taken by men.\n",
    "\n",
    "It is essential to continuously monitor social trends and enable AI systems to understand how these trends manifest locally. The use of proxy variables cannot always be avoided, similar to sensitive variables, as long as their necessity and significance for the intended purpose are carefully considered. Awareness of the proxy nature of particular variables allows for measures to prevent these variables from leading to legally prohibited gender discrimination. Identifying and preventing bias in an AI system early on is a key aspect of AI Governance in practice.\n",
    "\n",
    "Recognizing a proxy variable is particularly challenging when its discriminatory nature is subtle. A notable example is the recruitment tool developed by Amazon. This classification algorithm analyzed CV content, but due to differences in terminology used by men and women (e.g., \"women's rowing club\"), and because men had historically been selected more often, the algorithm assigned women a lower chance of suitability. As a result, candidates with identical qualifications could be treated differently based on gender. Due to the significant impact of such systems and the frequent oversight of these design flaws by developers, all AI systems used for recruitment are classified as high risk in the European Union and are subject to additional scrutiny and regulation.\n",
    "\n",
    "## Technical Approach...\n",
    "\n",
    "To identify proxy variables, we rely on a dataset's description. It is not always necessary to analyze the records within the dataset itself if the description is sufficiently detailed. We use descriptions similar to those found on Kaggle, focusing on a general overview of the content, the population, and the meaning of each column. Additionally, understanding the intended use of the dataset is crucial. If the actual data is available, further information can be gathered through dialogue with the user, although this is not essential for the initial version of the tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prototype tool is utilizing llamam-server () with either the Llama 3.1 8B or Mistal 7B model, with a Java wrapper that sends HTTP request in a specific style to ensure the result is in machine interpretable JSON format. The input data is a dataset description in the style as it commonly used on Kaggle. This is natural language text in a relative free format that typically briefly describes the intended goal of the dataset, the origin of the data and a brief description of each column, or variable, in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/rikdifos/credit-card-approval-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following input data from the link above: ",
    "<blockquote>\n",
    "_Context_\n",
    "Credit score cards are a common risk control method in the financial industry. It uses personal information and data submitted by credit card applicants to predict the probability of future defaults and credit card borrowings. The bank is able to decide whether to issue a credit card to the applicant. Credit scores can objectively quantify the magnitude of risk.\n",
    " \n",
    "Generally speaking, credit score cards are based on historical data. Once encountering large economic fluctuations. Past models may lose their original predictive power. Logistic model is a common method for credit scoring. Because Logistic is suitable for binary classification tasks and can calculate the coefficients of each feature. In order to facilitate understanding and operation, the score card will multiply the logistic regression coefficient by a certain value (such as 100) and round it.\n",
    " \n",
    "At present, with the development of machine learning algorithms. More predictive methods such as Boosting, Random Forest, and Support Vector Machines have been introduced into credit card scoring. However, these methods often do not have good transparency. It may be difficult to provide customers and regulators with a reason for rejection or acceptance.\n",
    "\n",
    "_Task_\n",
    "Build a machine learning model to predict if an applicant is 'good' or 'bad' client, different from other tasks, the definition of 'good' or 'bad' is not given. You should use some techique, such as vintage analysis to construct you label. Also, unbalance data problem is a big problem in this task.\n",
    "\n",
    "_Content & Explanation_\n",
    "There're two tables could be merged by ID:\n",
    "\n",
    "<pre>\n",
    "application_record.csv\n",
    "Feature name          Explanation                 Remarks\n",
    "ID                    Client number\t\n",
    "CODE_GENDER           Gender\t\n",
    "FLAG_OWN_CAR          Is there a car\t\n",
    "FLAG_OWN_REALTY       Is there a property\t\n",
    "CNT_CHILDREN          Number of children\t\n",
    "AMT_INCOME_TOTAL      Annual income\t\n",
    "NAME_INCOME_TYPE      Income category\t\n",
    "NAME_EDUCATION_TYPE   Education level\t\n",
    "NAME_FAMILY_STATUS    Marital status\t\n",
    "NAME_HOUSING_TYPE     Way of living\t\n",
    "DAYS_BIRTH            Birthday                    Count backwards from current day (0), -1 means yesterday\n",
    "DAYS_EMPLOYED         Start date of employment    Count backwards from current day(0). If positive, it means the person currently unemployed.\n",
    "FLAG_MOBIL            Is there a mobile phone\t\n",
    "FLAG_WORK_PHONE       Is there a work phone\t\n",
    "FLAG_PHONE            Is there a phone\t\n",
    "FLAG_EMAIL            Is there an email\t\n",
    "OCCUPATION_TYPE       Occupation\t\n",
    "CNT_FAM_MEMBERS       Family size\n",
    "</pre></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on processed input above, this is the JSON result we get back:\n",
    "<blockquote><pre>\n",
    "{\n",
    "    \"dataset\": {\n",
    "        \"title\": \"Credit Card Approval dataset\"\n",
    "    },\n",
    "    \"columns\": [\n",
    "        {\n",
    "            \"column\": \"ID\",\n",
    "            \"description\": \"Client Number\",\n",
    "            \"genderproxy\": \"no proxy\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"CODEGENDER\",\n",
    "            \"description\": \"Gender\",\n",
    "            \"genderproxy\": \"direct representation\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"FLAGOWNCAR\",\n",
    "            \"description\": \"Is there a car\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Owning a car could be a proxy for income, and income is sometimes correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"FLAGOWNREALTY\",\n",
    "            \"description\": \"Is there a property\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Owning real estate could be a proxy for income, and income is sometimes correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"CNTCHILDREN\",\n",
    "            \"description\": \"Number of Children\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Number of children could be a proxy for family structure, and family structure can be correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"AMTINCOMETOTAL\",\n",
    "            \"description\": \"Annual Income\",\n",
    "            \"genderproxy\": \"strong proxy\",\n",
    "            \"rationale\": \"Annual Income is directly correlated with gender due to societal factors such as job opportunities, wage gaps, and education levels.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"NAMEEDUCATIONTYPE\",\n",
    "            \"description\": \"Education Level\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Education level can be correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"NAMEFAMILYSTATUS\",\n",
    "            \"description\": \"Marital Status\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Marital status can be correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"NAMEHOUSINGTYPE\",\n",
    "            \"description\": \"Way of Living\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Way of living can be correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"DAYSBIRTH\",\n",
    "            \"description\": \"Age in days\",\n",
    "            \"genderproxy\": \"no proxy\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"DAYSEMPLOYED\",\n",
    "            \"description\": \"Duration of work in days\",\n",
    "            \"genderproxy\": \"no proxy\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"FLAGMOBIL\",\n",
    "            \"description\": \"Is there a mobile phone\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Mobile phone ownership can be correlated with income, and income is sometimes correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"FLAGOWNPHONE\",\n",
    "            \"description\": \"Is there a phone\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Phone ownership can be correlated with income, and income is sometimes correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"FLAGEMAIL\",\n",
    "            \"description\": \"Is there an email\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Email ownership can be correlated with income, and income is sometimes correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"JOB\",\n",
    "            \"description\": \"Job\",\n",
    "            \"genderproxy\": \"weak proxy\",\n",
    "            \"rationale\": \"Job can be correlated with gender, but the relationship is not consistent enough to be a strong proxy.\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"BEGIN_MONTHS\",\n",
    "            \"description\": \"Record month\",\n",
    "            \"genderproxy\": \"no proxy\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"STATUS\",\n",
    "            \"description\": \"Status\",\n",
    "            \"genderproxy\": \"no proxy\"\n",
    "        },\n",
    "        {\n",
    "            \"column\": \"TARGET\",\n",
    "            \"description\": \"Risk user are marked as '1', else are '0'\",\n",
    "            \"genderproxy\": \"no proxy\"\n",
    "        }\n",
    "    ],\n",
    "    \"rationale\": \"The variables in this dataset were assessed for their potential as proxies for gender. Several variables, such as income and education level, are known to be correlated with gender, but the relationship is not consistent enough to be considered strong proxies. Other variables, like the presence of a car, a phone, or an email, could potentially function as weak proxies for gender due to their correlation with income, but the relationship is not consistently strong.\"\n",
    "}\n",
    "</pre></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
